# Product Requirements Document: FourSight

**Version:** 1.0
**Date:** 2025-11-03
**Author:** BMad Product Manager
**Status:** Draft

---

## 1. Introduction

### 1.1. Problem Statement
Professionals, particularly business strategists, make critical decisions daily but often lack the diverse insights needed to do so with high confidence. The core problem is the limited analytical scope they employ, which can lead to bias and suboptimal outcomes.

### 1.2. Proposed Solution
FourSight is a multi-agent decision analysis system that provides users with a team of AI consultants. It intelligently selects and executes four relevant decision-making frameworks in parallel, synthesizes the results, and delivers a single, actionable recommendation with a clear confidence score. This ensures a comprehensive, multi-faceted analysis, empowering users to make better-informed decisions.

### 1.3. Target Audience
The primary user is a **business strategist** who regularly needs to make complex decisions to foster business growth. The system is also designed to be intuitive for any professional facing a significant decision.

### 1.4. Hierarchy of User Goals
When using FourSight, the primary user's goals are, in order of importance:
1.  **Generate a definitive recommendation** (6/10)
2.  **Explore unforeseen risks** (3/10)
3.  **Validate a preconceived notion** (1/10)

### 1.5. Scope
This document outlines the requirements for the Minimum Viable Product (MVP) to be delivered for the Google Cloud Run Hackathon.

---

## 2. User Requirements & Features

### 2.1. User Authentication and Dashboard
Users must be able to create an account and log in. Upon logging in, they will be presented with a dashboard.

-   **FR-2.1.1: User Authentication:** The system shall support user registration and login via email/password.
-   **FR-2.1.2: User Dashboard:** The dashboard shall display the following key metrics:
    -   Total number of decisions analyzed.
    -   The user's average confidence level across all recommendations.
    -   A visualization of the most frequently used frameworks.
    -   Total LLM costs incurred by the user to date.

-   **FR-2.1.3: Global Navigation:** The application shall feature a two-tiered navigation system. A main top bar will contain the centered logo and right-aligned user/settings icons. Below it, a floating navigation bar will provide button-based access to the main pages (Dashboard, New Analysis, History, etc.).

### 2.2. Core Decision Analysis Workflow
This is the primary user journey for analyzing a new decision.

-   **FR-2.2.1: Query Input:** The user shall be able to input their decision query into a simple text interface.
    -   **FR-2.2.1.1: Example Queries:** Below the chat input box, the UI shall display four example queries in a 2x2 grid of cards. These examples are generated by the Orchestrator to guide the user on how to formulate a good query.
-   **FR-2.2.2: Framework Ranking:** The Orchestrator agent shall analyze the query and rank all 10 available frameworks based on relevance, providing a score and rationale for each.
-   **FR-2.2.3: Framework Selection & Validation:**
    -   The UI will display the top 4 recommended frameworks as interactive cards in a 2x2 grid, showing the framework title and rationale.
    -   The remaining 6 frameworks will be displayed below as simple, color-coded buttons with only their titles.
    -   The user can deselect a recommended framework by clicking an 'X' icon on its card. The deselected framework card will be removed, leaving a gap, and it will reappear as a button in the list below.
    -   The user can hover over any of the non-selected framework buttons to view its rationale in a pop-up card.
    -   The user can click on a framework button to select it, which will fill an empty slot in the 2x2 grid. Clicking does nothing if the grid is full.
    -   A "Start Analysis" button will only become active when exactly four frameworks are selected in the 2x2 grid.
-   **FR-2.2.4: Iterative Q&A Session & A2A Communication:**
    -   After framework validation, any of the four selected agents can initiate a Q&A session if more context is needed.
    -   **A2A Flow:** Framework agents do not communicate directly with the user. When an agent requires more information, it sends the question to the Orchestrator (A2A). The Orchestrator then surfaces this question within the specific agent's chat interface in the UI.
    -   The user's response, while entered in the agent's UI, is sent directly to the Orchestrator.
    -   The Orchestrator updates the "shared context" with the user's answer and broadcasts this update to all four active agents.
    -   Each agent is limited to asking a maximum of three questions.
    -   Agents will process all information within a user's response, even if it was not explicitly solicited by the question.
-   **FR-2.2.5: Parallel Execution ("YOLO Mode"):**
    -   Once the Q&A session is complete (i.e., the last question from any agent has been answered), the Orchestrator will trigger the parallel execution of all four agents.
    -   If an agent still lacks sufficient information after its three questions, it must proceed with the analysis regardless.
-   **FR-2.2.6: Synthesized Recommendation:**
    -   The Orchestrator will synthesize the outputs from the four agents into a single, final recommendation.
    -   The output must be prominently highlighted and structured as follows:
        1.  **Recommendation & Confidence Score:** The direct answer and a confidence level (e.g., High, Medium, Low) displayed at the top.
        2.  **Executive Summary:** A brief paragraph summarizing the findings.
        3.  **Key Supporting Points:** Bullet points of critical insights.
        4.  **Key Risks and Contradictions:** A section calling out risks or conflicting findings.
-   **FR-2.2.7: Agent Drill-Down:** The user must be able to view the detailed, individual analysis from each of the four framework agents.
-   **FR-2.2.8: Explicit Information Gaps:** If an agent performs its analysis with incomplete data (per FR-2.2.5), its final report must include a clear statement identifying the missing information and explaining how this gap could potentially affect the outcome.

### 2.3. History and Retry Functionality
Users must be able to access and learn from their past analyses.

-   **FR-2.3.1: History Page:** The application shall have a history page that displays a chronological list of all past decision analyses, with each analysis presented on a separate card.
-   **FR-2.3.2: History Card Details:** Each card in the history list shall display:
    -   The initial decision query.
    -   The final synthesized recommendation and confidence level.
    -   The timestamp of the analysis.
    -   The total LLM cost for the analysis, displayed in dollars.
-   **FR-2.3.3: Retry with Other Frameworks:**
    -   Each history card shall feature a "Retry" option.
    -   Activating this option will initiate a new analysis, pre-populated with the original query.
    -   The user will be taken to the framework selection screen, where the six frameworks that were *not* used in the original analysis will be ranked and available for selection.

---

## 3. Non-Functional Requirements

-   **NFR-3.1: Performance:** The parallel execution of agents should be efficient to minimize user wait times. The UI must remain responsive during analysis.
-   **NFR-3.2: Scalability:** The serverless architecture on Google Cloud Run must scale automatically to handle varying loads.
-   **NFR-3.3: Security:** User authentication must be secure. All inter-service communication and API endpoints must be protected. Secrets will be managed via Google Secret Manager.
-   **NFR-3.4: Observability:** The system must have centralized logging and monitoring to track performance, errors, and LLM token usage.

---

## 4. Out of Scope for MVP

-   Advanced features like file uploads for additional context.
-   Integration with external data sources or third-party tools.
-   Administrator-level controls for managing frameworks or agents.
-   User-configurable Q&A session length.

---
